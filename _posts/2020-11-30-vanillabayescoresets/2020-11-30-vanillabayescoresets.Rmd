---
title: "vanillabayescoresets"
description: |
  An R package for implementing Uniform and Hilbert Frank-Wolfe Bayesian Coresets for logistic regression
author: 
  - name: Federica Zoe Ricci
date: "11/30/2020"
output:
  distill::distill_article:
    self_contained: false
    toc: false
    css: ../../theme.css
preview: ../../img/plot_vanilla.jpg
draft: false
tags: [R, algorithms, bayesian, regression, package]
categories:
  - R package
  - Logistic regression
  - Bayesian
  - Coresets

---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(vanillabayescoresets)
library(ggplot2)
```

## R package

As part of a course project, [Rachel Longjohn](https://rlongjohn.github.io/) and I have written an R package named `vanillabayescoresets` for implementing [Bayesian coresets](https://arxiv.org/abs/1710.05053) for binary logistic regression data in R. 
The functions provided in our package are simplified versions (reason for which the _vanilla_) of some of the functions available in Python through the packages [lrcoresets](https://bitbucket.org/jhhuggins/lrcoresets/src/master/coresets/) and [bayesian-coresets](https://github.com/trevorcampbell/bayesian-coresets), created by Jonathan H. Huggins and Trevor Campbell. 

In this article you can read some information about `vanillabayescoresets`. The [package](https://github.com/federicazoe/vanillabayescoresets) and its [vignette](https://federicazoe.github.io/vanillabayescoresets/) are available on GitHub.


### A little background, first!

Fitting Bayesian models on large datasets using standard methods such as Markov Chain Monte Carlo can be computationally very expensive, and may reveal unfeasible in applications that require fast results. 

Several modifications of MCMC, as well as alternative methods like Variational Inference, have been developed for making inference on the full dataset more scalable.

Bayesian coresets are a different approach that achieves scalability by focusing on a pre-processing step, aimed at delivering a small, weighted subsample of the full data on which to run any inference algorithm.

### What is implemented in this package?


Through `vanillabayescoresets`' functions you will be able to:

* obtain uniform or Hilbert Frank-Wolfe coresets for binary logistic regression data, setting the size of the coreset along with other construction parameters.
* plot the coreset selected over the full dataset, for the case with two covariates, to visualize which datapoints are selected and what weight is assigned to them by the two methods.
* quickly generate synthetic binary data, customizing the size of the sample, the number and the generative model of covariates, and the model parameters.

The functions in the package are:

* `simulate_logit_data()`: simulate observations and covariates from a logistic regression model
* `get_coreset_uniform()`: implements the uniform coresets for binary logistic data, as detailed in Algorithm 1 of [J. Huggins et al. (2016)](https://arxiv.org/abs/1605.06423).
* `get_coreset_frankwolfe()`: implements the Hilbert Frank-Wolfe coresets for binary logistic data, as detailed in Algorithms 2 and 3 and Sections 4.2 and 5 of [T. Campbell and T. Broderick (2019)](https://arxiv.org/abs/1710.05053). 
* `visualize_coresets()`: plots the datapoints against **two** continuous covariates, labels them as either failures (i.e. y = -1) or successes (i.e. y = 1), marks the data points selected in the coreset and represents their weight as the point's size.
 

### Demonstration

The following code demonstrates how to obtain and visualize a Frank-Wolfe coreset for a logistic regression of a binary variable on 2 covariates. 

```{r, echo=TRUE, eval=FALSE}

library(vanillabayescoresets)

data <- simulate_logit_data()
coreset_frankwolfe <- get_coreset_frankwolfe(data$x, data$y)
visualize_coresets(coreset_frankwolfe, data$x, data$y)

```

```{r, echo=FALSE, fig.align='center', fig.cap = "Bayesian coresets for logistic regression obtained with the Frank-Wolfe method", preview=FALSE, layout="l-body-outset"}

# save the data, to save time when the article compiles
# save(coreset_frankwolfe, file = here::here("data", "coreset_frankwolfe.Rdata"))
# save(data, file = here::here("data", "simulated_logit_data.Rdata"))

load(file = here::here("data", "coreset_frankwolfe.Rdata"))
load(file = here::here("data", "simulated_logit_data.Rdata"))
y <- data$y
x <- data$x
visualize_coresets(coreset_frankwolfe, x, y)

```

```{r echo=FALSE, eval=FALSE}

# Create plot for preview image

# x <- x[, -3]
# # get coreset weights
# weights <- coreset_results$weights
# 
# # format data
# mydata <- cbind(y, x)
# colnames(mydata) <- c("y", paste0("V", 1:2))
# mydata <- as_tibble(mydata)
# mydata <- mydata %>%
#   mutate(weight = weights,
#          selected = .data[["weight"]] > 0,
#          true_label = case_when(.data[["y"]] == 1 ~ "success",
#                                 .data[["y"]] != 1 ~ "failure"))
# 
# # base plot
# ggplot(mydata) +
#   aes(x = V2, y = V3) +
#   geom_point(aes(fill = true_label), shape = 21, color = "blue",
#              stroke = 0.25, alpha = 0.25, size = 1.5) +
#   scale_fill_brewer(palette = "Set2") +
#   geom_point(data = filter(mydata, selected == TRUE),
#              fill = "black",
#              shape = 21,
#              stroke = 0.01,
#              aes(size = weight)) +
#   labs(x = expression(x[1]),
#        y = expression(x[2])) +
#   theme_void() +
#   guides(fill = FALSE,
#          size = FALSE)

```





